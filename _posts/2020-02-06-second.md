# How to approach your NLP problem

In this post I'll try to give you some general hint for your solving your NLP tasks.

## Surf!

When you are trying to solve an NLP task, generally speaking, I always suggest to first have a look what's into "the wild". 
**Surfing** on the web using the correct keywords plays an important role here. 
In this field there is a tremendous sparsity of terms and synonyms, as well as an huge amount of sub-fields and sub-tasks, 
bringing the number of possible combinations of this terms to explode.

Depending on your knowledge of the field/litterature and your "scraper" skills, you can find a lot of usefull information, and also code.
At the end of the day, if you are lucky, you have to adapt and run some code to be able to run on your data and provide some result.
In the worst case scenario you will have to implement by scratch what you find out in a paper or post.

My personal preferences and suggestions are [Papers with code](https://paperswithcode.com/area/natural-language-processing) and [Kaggle](https://www.kaggle.com/) but also 
[Github](https://github.com/) and [Medium](https://medium.com/) have good references. 

A very good reference for a lot of NLP datasets is the [The Big Bad NLP Database](https://quantumstat.com/dataset/dataset.html)

## Keep your portfolio and pipeline.

Whenever you find a solution try to keep the code as much **reusable** as possible, so that you can have your set of sub-modules always ready for you!
This is always a dream, however sometimes could happen that your code cannot be completely re-used either because it's not clean or for other reasons (e.g company code integration) .
In any case there are general high level procedures which can be applied to the same categories of problems, for which you can have your **pipeline** ready!

Think about a general NLP classification problem, or a regression one. What you usually do, before even going into building sci-fi models and super-deep neural networks, is to *analyze* and properly *clean* the data.

So, what I suggest are two basic pipelines:
1. **Data cleaning**
2. **Data analysis**


### Data cleaning pipeline

A data cleaning pipeline can obviously be built in any way. However, here are some hints for a good pipeline:
1. Standardize your input. Use, for example, always a CSV file with two columns at least (text + target). This way you have a standard procedure for loading the data.

2. I highly suggest the usage of **pandas** for these tasks, since it offers pre-built functions and customizable optimized procedures for any cleaning/analysis problem. A lot of predefined cleaning procedures can also be found on **nltk** and **spacy**, two very common and used NLP python libraries.

3. Since this pipeline will be re-used across projects, try to think about a good way of rule-injection for cleaning your data. A good start could be a dictionary (or a list), from which rules (cleaning functions) are loaded and applied to the pandas DataFrame. This way you can write additional rules and easily enhance the cleaning pipeline.

4. If the pipeline starts becoming bigger and bigger, try to modularize it and find a way to configure the pipeline through a YML/JSON file, so you can easily setup everything with few steps.

5. Sometimes a little dirt on your data could be acceptable, let me explain. Running several experiments, I found out that sometimes not removing stopwords can help. I was able to get +1% in Accuracy on a binary classification task, keeping stopwords. So, I would also suggest to try some experiments where you drop some of the cleaning procedures.

We will see in one of the next posts an example of data cleaning pipeline.

### Data analysis pipeline

Building a general data analysis pipeline for NLP tasks is not that straightforward as building a data cleaning one.
A good Exploratory Data Analysis requires task knowledge and problem-specific analaysis. For example, in a [Kaggle challenge](https://www.kaggle.com/doomdiskday/full-tutoria-eda-to-ensembles-embeddings-zoo) my EDA also kept into account hashtag, locations and keywords, which are specific to that problem.

However a basic data analysis can be a start for a pipeline. Just running on text-labels statistics you can have a good insight on the difficulty of the problem. 
As for the data-cleaning pipeline, I highly suggest to **standardize** the input and use **pandas** together with **matplotlib** and **seaborn**, which are two major libraries for visualization.

Also in this case we will see an example in one of the next posts.

