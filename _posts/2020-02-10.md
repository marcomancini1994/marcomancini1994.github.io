# Clean your data: A practical pipeline!

## Data Cleaning pipeline

In this notebook we are gonna see how to create a **data cleaning pipeline for a NLP binary classification task**.
The objective is to understand bow to build an **extendible** and **reusable** pipeline for cleaning your data. 

## The problem: [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)

We are gonna start from a basic **binary classification problem**, which is also a kaggle challenge. 

The problem consists into categorizing a tweet into a "real disaster"(class 1) or "fake" (class 0).
The input training data consists of three columns: text, keyword and location, together with the Y binary label.
The test data consist of the same feature set, obviously witouth the Y.



```python
import pandas as pd
import nltk
import os
import logging
import string
import re 
import yaml
```

## Cleaning functions 

Let's define a **structure** for the cleaning functions, so that we can easily inject rules by adding a function here below and configuring properly the pipeline.

Pandas allows us to exploit **lambda functions** on any column (field). 
This mean that a one-liner can be enough to cycle all over your data and apply your cleaning function. 

Writing a function which will be given as input to a lambda means that the input will be a **single element**, the lambda will make the rest. 

Here I will write the cleaning procedures for the following cleaning tasks:
1. Stopwords removal
2. URL removal
3. HTML removal
4. Emoji removal
5. Punctuation removal
6. Word lowering


```python

def remove_stopwords(text):
    if text is not None:
        tokens = [x for x in word_tokenize(text) if x not in stop]
        return " ".join(tokens)
    else:
        return text

def remove_URL(text):
        url = re.compile(r'https?://\S+|www\.\S+')
        return url.sub(r'',text)

def remove_html(text):
        html=re.compile(r'<.*?>')
        return html.sub(r'',text)

def remove_emoji(text):
        emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)


def remove_punct(text, what=None):
        table=str.maketrans('','',string.punctuation)
        return text.translate(table)

def lower_text(text):
    return text.lower() if text is not None else text
    
```

Now we need a **Pipeline**, which can range over a configuration setting and handle the cleaning process.
The configuration will be, for now, as simple as a dictionary having the **rule name as key** and a dictionary of parameters as value (or None if no parameters are needed).



```python
class CleaningPipeline(object):
    
    def load_config(self, config):
        """
        This method loads a configuration cleaning object. 
        We expect either a string pointing to the configuration YML having the fields described in the below
        section or a dictionary with those information!
        """
        
        if isinstance(config, dict):
            conf = config
        elif isinstance(config, str):
            if not os.path.exists(config): 
                logging.error("[!] The configuration file does not exists!")
                return None, None, None
            else:
                try:
                    conf = yaml.load(open(config))
                except Exception as e:
                    logging.error("[!] There was a problem while loading the configuration!")
                    logging.error("[!] Exception: {}".format(e))
                    return None, None, None
        
        if not isinstance(conf, dict):
            logging.error("[!] The configuration cannot be loaded as dictionary for some reason!")
            logging.error("[!] Check the input, should be either a YML defining a dictionary \
                          with 'data_output', 'data_input' and 'cleaning_functions' as keys or directly \
                          a dictionary like that")
            return None, None, None
                          
        if len(set(["data_input", "data_output", "cleaning_functions"]).difference(set(conf.keys()))) > 0:
                logging.error("[!] The configuration accepts only the three keys \
                              'data_input', 'data_output' and 'cleaning_functions'")
                

        return conf.get("data_input",None), conf.get("data_output"), conf.get("cleaning_functions")
                
    def load_data(self, data_path):
        """
        Load a dataframe and expect a csv with at least one column.
        The column must have the name "text", in order to be properly loaded for cleaning
        """
        df = None

        if not os.path.exists(data_path):
            logging.error("[!]Data input does not exists")
            return 0

        try:
            df = pd.read_csv(data_path)
            if "text" not in df.columns:
                raise Exception

        except Exception as e:
            logging.error("[!]For some reason the input data cannot be read!\n[!]Please make sure to have a valid csv with ")

        if df is not None:
            logging.info("[>]Data correctly loaded")
        return df
    
    def check_configuration(self, config):
        """
        We check each function can be correctly called, before cleaning!
        """
        try:
            for rule, conf in config.items():
                if conf is not None:
                    eval(rule)("test", conf)
                else:
                    eval(rule)("test")
        except Exception as e:
            logging.error("[!] Function {} not found! Cleaning process aborted!".format(rule))
            logging.error("[!] Exception: {}".format(e))
            return -1
            
    
    def clean(self,configuration, df, dump_path=None):
        """
        We expect a configuration dictionary of the following form:

        {
            "rule_name": {"rule_parameter_1": "1", "rule_parameter_2": "2",...}
            "rule_name2": None, #if no parameter, just pass None.
        }
        """
        #if self.check_configuration(configuration) == -1:
        #   return df
        for rule, conf in configuration.items():
            if conf is not None:
                df['text']=df['text'].apply(lambda x : eval(rule)(x, conf))
            else:
                df['text']=df['text'].apply(lambda x : eval(rule)(x))
        
        if dump_path is not None:
            _dir = os.path.split(dump_path)[0]
            os.makedirs(_dir, exist_ok=True)
            self.dump(df, dump_path)
            
        return df
    
    def dump(self, df, path, index=False):
        df.to_csv(path, index=index)
    
    def run_from_config(self, config_path):
        data_in, data_out, cleaning_functions = self.load_config(config_path)
        df = self.load_data(data_in)
        return self.clean(cleaning_functions, df, data_out)
    
```


```python
_k = ["lower_text", "remove_punct", "remove_URL", "remove_html", "remove_emoji" ]
conf = {k: None for k in _k}


rm = CleaningPipeline()
df = rm.load_data(data_path="data/train.csv")

dump_path="data/train_clean.json"
df = rm.clean(conf, df,dump_path)
print(df.head())
```

       id keyword location                                               text  \
    0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   
    1   4     NaN      NaN              forest fire near la ronge sask canada   
    2   5     NaN      NaN  all residents asked to shelter in place are be...   
    3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   
    4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   
    
       target  
    0       1  
    1       1  
    2       1  
    3       1  
    4       1  


## Configuration through YML (optional)

The above code is enough for If you want to export the code and embed it into your code.
However, as suggested in my [first post](https://marcomancini1994.github.io/2020/02/06/second.html), you can keep this pipeline growing and growing at each experiment you run and configure it through a YML/Json file, so that you can use the pipeline to process your data wherever they are!

Here we are going to show a very simple configuration strategy, where we define in a YML object the set of functions we want to run through our pipeline, the input data path and the output data path. Then the data cleaning procedure will be launched and data processed under your output folder.

The YML input will be something like this:

```yml
data_input: "data/train.csv"

data_output: "data/processed_train.csv"

cleaning_functions:
    lower_text: null
    remove_punct: null
    remove_URL: null
    remove_html: null
    remove_emoji: null

```


```python
config = {
    "data_input": "data/train.csv",
    "data_output": "data/processed_train.csv",
    "cleaning_functions": 
        {"lower_text": None,
        "remove_punct": None,
        "remove_URL": None,
        "remove_html": None,
        "remove_emoji": None}
}
```


```python
pipeline = CleaningPipeline()
df = pipeline.run_from_config(config)
```

You can easily write an **argparse** to take the config file path from your local machine, and that's it!

In the next notebook we will see an Exploratory Data Analysis
