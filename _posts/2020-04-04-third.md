# Kaggle Challenge: [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)

## The problem

Twitter has become an important communication channel in times of emergency.
The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. 
Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).

But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:

![](https://storage.googleapis.com/kaggle-media/competitions/tweet_screenshot.png)


The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.

In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.

Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.

The challenge allows only **Auto ML** submissions, however in this post we're gonna use standard ML and AI techniques, witouth keeping into account AutoML

## Sentiment Classification

### Objective

Sentiment analysis is a common use case of NLP where the idea is to classify the tweet as positive, negative or neutral depending upon the text in the tweet.
So, in a ML setting, this can be tracted as a multi-class classification problem, where we have a Positive label (1), a Neutral label (0) and a Negative label (-1) for each sample (tweet).

Sentiment analysis allows businesses to identify customer sentiment toward products, brands or services in online conversations and feedback, so it can lead to useful insights in a production environment.
For example, an e-commerce customer can use sentiment analysis to automatically analyze 4,000+ reviews about their product, and discovered that customers were happy about their pricing but complained a lot about their customer service (like in the image below)
![](https://pcdn.piiojs.com/i/kqctmw/vw,671,vh,0,kc,1,r,1,pr,2,wp,1/https%3A%2F%2Fmonkeylearn.com%2Fstatic%2Fimg%2Fsentiment-analysis%2Fsentiment-analysis-example-new%402x.png)

### Types of Sentiment Analysis

In general, we can have several granularities of sentiment. For example, if polarity precision is important to your business, you might consider expanding your polarity categories to include:
- Very positive
- Positive
- Neutral
- Negative
- Very negative

In addition, in the litterature we can find several types of sentiment analysis:
#### Emotion detection
Emotion detection aims at detecting emotions, like happiness, frustration, anger, sadness, and so on.
Many emotion detection systems use lexicons (i.e. lists of words and the emotions they convey) or complex machine learning algorithms.
One of the downsides of using lexicons is that people express emotions in different ways. Some words that typically express anger, like bad or kill (e.g. your product is so bad or your customer support is killing me) might also express happiness (e.g. this is bad ass or you are killing it).

#### Aspect-based Sentiment Analysis
Usually, when analyzing sentiments of texts, let’s say product reviews, you’ll want to know which particular aspects or features people are mentioning in a positive, neutral, or negative way. That's where aspect-based sentiment analysis can help, for example in this text: "The battery life of this camera is too short", an aspect-based classifier would be able to determine that the sentence expresses a negative opinion about the feature battery life.

#### Multilingual sentiment analysis
Multilingual sentiment analysis can be difficult. It involves a lot of preprocessing and resources. Most of these resources are available online (e.g. sentiment lexicons), while others need to be created (e.g. translated corpora or noise detection algorithms), but you’ll need to know how to code to use them.

Alternatively, you could detect language in texts automatically, then train a custom sentiment analysis model to classify texts in the language of your choice.

### How does it work?
Sentiment analysis uses various Natural Language Processing (NLP) methods and algorithms.

The main types of algorithms used include:
- Rule-based systems: Perform sentiment analysis based on a set of manually crafted rules (e.g check "bad" words and "good" words frequencies).
- Automatic systems: Rely on machine learning techniques to learn from data. (e.g Tokenization with TFIDF + MLP classifier, Word-embeddings + log-reg, ensembles..)
- Hybrid systems: Combine both rule-based and automatic approaches.

### Evaluation
In order to evaluate a sentiment classification algorithm we can use several techniques.
For example we could use Accuracy, as showed in the below image:

![Accuracy Calculation](https://cdn-images-1.medium.com/max/800/1*5XuZ_86Rfce3qyLt7XMlhw.png)


However, since it could happen that negative and positive labels have a different distribution in the dataset, a more appropriate 
evaluation metric is the so called F1-Score. 
The F1 score is the armonic mean between precision and recall, so is calculated as shown in the below image:

![](https://i.stack.imgur.com/U0hjG.png)


## My Kaggle Approach!

In this challenge the sentiment is just classified as negative or positive (2-class classification problem) and here I'm gonna briefly describe the approach I used to reach an ~83% F1.
We'll start from EDA (Exploratory Data Analysis), then we show how I cleaned the data and, at the end, several models are shown and compared.

### Exploratory Data Analysis
In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. 
A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.

For this challenge I've performed the following EDA:
- General dataset infos
  - Number of samples
  - Data Columns
  - Class Label Distributiom
- Text analysis
  - Number of characters in tweets
  - Number of words in a tweet
  - Average word lenght in a tweet
  - Word distribution
  - Hashtag Analysis
  - KW and Location Analysis

For the code, I suggest to take a look directly to my [Kaggle Notebook](https://www.kaggle.com/c/nlp-getting-started).

### Data Cleaning

As I explained in [previous post](https://marcomancini1994.github.io/2020/02/10/second.html), cleaning the data is a standard practice in NLP, since the datasets are mostly coming from real interactions.

The data cleaning pipeline I built for this challenge is composed by:
- stopwords removal
- URL removal
- HTML removal
- emoji removal
- punctuation removal

### Evaluating the models

For this challenge I decided to try a bunch of very basic models (Baselines) and some more advanced ML and AI approach, also using the most recent Sentence Embeddings.

- Baseline Models 
  - Majority model 
  - tfidf + logreg
- ML + AI
  - Gradient Boosting
  - Simple RNN
  - Glove Bi-LSTM
  - BERT + sigmoid
  - Ensemble (BERT + 10 shallow classifiers)

#### Baseline Models
Like for every ML problem, we always need to have some baseline approach. 
My suggestions, for a general classification problem, is to use a Majority Model and basic ML approach like TFIDF featurization + Logistic Regression.

#### Majority Voting model
This model is one of the most stupid model you can find in the litterature.
What it does is to assign to each **test sample** the label with the highest frequency in the **training set**.
Obiously the implementation is super-straightforward:
```python
def test_majority_model(test_path="/kaggle/input/nlp-getting-started/test.csv"):
    
    my_df = pd.read_csv(test_path)
    
    res = my_df[['id']]
    res['target'] = 1
    
    res.to_csv("res_majority.csv", index = False)
    return res
```

#### TFIDF + LogReg
This BL model uses one of the most old and used tokenization technique, the TFIDF.

**Tf-idf** stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It is calculated using two factors, **TF** and **IDF**:
- TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:

TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).

- IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:

IDF(t) = log_e(Total number of documents / Number of documents with term t in it).

- TFIDF: TF * IDF

Given this featurization methodology, we then apply a **Logistic Regrression** on the top, in order to classify the text.
This is one of the firsts ML models appeared, and also one of the most used for these kind of problems, since it is super easy and fast to train and can also give good BL scores.
In order to understand it better, I recommend to follow [this](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) guide. 

The implementation is super-straightforward and can be done using **sklearn**, as here below:

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

"""
Build a baseline TFIDF + LOGREG based just on text
"""
def build_tfidf_logreg(df):
    my_df = df[['text','target']]
    x_features = my_df.columns[0]
    x_data = my_df[x_features]
    Y = my_df["target"]

    x_train, x_validation, y_train, y_validation = model_selection.train_test_split(
        x_data.values, Y.values, test_size=0.2, random_state=7)
    
    # configure TfidfVectorizer to accept tokenized data
    # reference http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/
    tfidf_vectorizer = TfidfVectorizer(
        analyzer='word',
        tokenizer=lambda x: x,
        preprocessor=lambda x: x,
        token_pattern=None)

    lr = LogisticRegression()
    tfidf_lr_pipe = Pipeline([('tfidf', tfidf_vectorizer), ('lr', lr)])
    tfidf_lr_pipe.fit(x_train, y_train)
    
    return tfidf_lr_pipe

def test_tfidf_logreg(model, test_path="/kaggle/input/nlp-getting-started/test.csv"):
    
    my_df, res_df = read_test(test_path="/kaggle/input/nlp-getting-started/test.csv")
    
    #x_features = my_df.columns[0]
    x_data = my_df["text"].values

    preds = model.predict(x_data)
    
    #dump_preds(res_df, preds, out="res_tfidf_logreg4_0.csv")
    
    return res_df
```

#### Gradient Boosting

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. (Wikipedia definition)

For a better understanding I suggest the [following medium guide](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)

For what concerns the implementation, again we can use **sklearn**:
```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

X_train, y_train = split_data(df)

test_df, res_df = read_test(test_path="/kaggle/input/nlp-getting-started/test.csv")
X_test = split_data(test_df, _t=False)

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', GradientBoostingClassifier(n_estimators=100)),
                     ])
#text_clf.fit(X_train, y_train)
#predicted = text_clf.predict(X_test)
#dump_preds(res_df, predicted, out="submission.csv")
#0.66462
```
#### The neural way

As explained above, I also decided to make use of neural networks and sentence embeddings.
In particular, since we're tracting sentences (which can be considered timeseries of words), I tried several Recurrent Neural Networks, like RNN and LSTM. Moreover, in these last years, the usage of hidden representations exploded, bringing to the community several DNNs pre-trained on a huge amout of data.
They born on a word-level representation but, with the years, the research community was able to develop also sentence embedding techniques.

The most common ones are:
- Word2Vec
- FastText
- Universal Sentence Encoder
- Glove
- XLNet
- BERT
- Roberta
- Albert
- Distilbert

In my original notebook you can find most of them used for this sentiment classification task, take a look!


#### Ensembling!

Once good performances have been reached using several models, a good and really used technique to try to beat the performance is the **ensembling**.
With ensembling several models are combined together and predictions are based on the output of all the models. 

The 3 main techniques used for ensembling are the following:
- **Bagging**: Bagging stands for bootstrap aggregation. One way to reduce the variance of an estimate is to average together multiple estimates. Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses voting for classification and averaging for regression.

- **Boosting**: Boosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners to weighted versions of the data. More weight is given to examples that were misclassified by earlier rounds. The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. 

- **Stacking**: Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features.

Take a look to my notebook to check how I used **stacking** for this sentiment classification problem!
